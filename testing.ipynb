{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.buffers as buffers\n",
    "import utils.stream as stream\n",
    "import importlib\n",
    "import pickle\n",
    "import os\n",
    "from math import ceil, log2\n",
    "import utils.invert_index as invert_index\n",
    "import numpy as np\n",
    "import utils.query_runner as query_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamaño máximo para un buffer...\n",
    "MAX_DICT_SIZE = 1000\n",
    "# stream lee de listas de tamaño...\n",
    "LIMIT = 400\n",
    "# almacenar outputs temporales en...\n",
    "main_path = \"D:/Documents-D/temp/\" \n",
    "dir1 = main_path + \"dir1/\"\n",
    "dir2 = main_path + \"dir2/\"\n",
    "# ubicación final del índice invertido...\n",
    "index_filename = \"index.pikl\"\n",
    "final = main_path + \"inverted_index/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.buffers' from 'c:\\\\Users\\\\Windows 11\\\\Documents\\\\UTEC\\\\Testing\\\\index_merge\\\\utils\\\\buffers.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(stream)\n",
    "importlib.reload(invert_index)\n",
    "importlib.reload(buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document lyrics/4_out_of_5.txt has been preprocessed!\n",
      "Document lyrics/505.txt has been preprocessed!\n",
      "Document lyrics/arabella.txt has been preprocessed!\n",
      "Document lyrics/batphone.txt has been preprocessed!\n",
      "Document lyrics/cornerstone.txt has been preprocessed!\n",
      "Document lyrics/dance_little_liar.txt has been preprocessed!\n",
      "Document lyrics/no1_party_anthem.txt has been preprocessed!\n",
      "Document lyrics/one_for_the_road.txt has been preprocessed!\n",
      "Document lyrics/when_the_sun_goes_down.txt has been preprocessed!\n"
     ]
    }
   ],
   "source": [
    "with open(\"stoplist.txt\") as file:\n",
    "    stoplist = [line.rstrip().lower() for line in file]\n",
    "\n",
    "# test files\n",
    "files = os.listdir(\"lyrics/\")\n",
    "files = [\"lyrics/\" + file for file in files]\n",
    "doc_count = len(files)\n",
    "\n",
    "# generate stream\n",
    "ss = stream.Stream(files, stoplist, LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construcción del índice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:/Documents-D/temp/dir2/f0.pkl', 'D:/Documents-D/temp/dir2/f1.pkl', 'D:/Documents-D/temp/dir2/f2.pkl', 'D:/Documents-D/temp/dir2/f3.pkl', 'D:/Documents-D/temp/dir2/f4.pkl', 'D:/Documents-D/temp/dir2/f5.pkl', 'D:/Documents-D/temp/dir2/f6.pkl', 'D:/Documents-D/temp/dir2/f7.pkl', 'D:/Documents-D/temp/dir2/f8.pkl', 'D:/Documents-D/temp/dir2/f9.pkl', 'D:/Documents-D/temp/dir2/f10.pkl', 'D:/Documents-D/temp/dir2/f11.pkl', 'D:/Documents-D/temp/dir2/f12.pkl', 'D:/Documents-D/temp/dir2/f13.pkl', 'D:/Documents-D/temp/dir2/f14.pkl', 'D:/Documents-D/temp/dir2/f15.pkl', 'D:/Documents-D/temp/dir2/f16.pkl', 'D:/Documents-D/temp/dir2/f17.pkl', 'D:/Documents-D/temp/dir2/f18.pkl', 'D:/Documents-D/temp/dir2/f19.pkl', 'D:/Documents-D/temp/dir2/f20.pkl', 'D:/Documents-D/temp/dir2/f21.pkl', 'D:/Documents-D/temp/dir2/f22.pkl', 'D:/Documents-D/temp/dir2/f23.pkl', 'D:/Documents-D/temp/dir2/f24.pkl']\n"
     ]
    }
   ],
   "source": [
    "# SPIMIndexConstruction\n",
    "files_to_merge = []\n",
    "n = 0\n",
    "while not ss.empty:\n",
    "    fn = invert_index.spimi_invert(dir2, ss, n, MAX_DICT_SIZE)\n",
    "    files_to_merge.append(fn)\n",
    "    n += 1\n",
    "ss.end()\n",
    "print(files_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "levels = ceil(log2(n))\n",
    "for i in range(levels):\n",
    "    last_file = 0 # index of last file\n",
    "    dir1, dir2 = dir2, dir1\n",
    "    for j in range(ceil(n / pow(2, i + 1))):\n",
    "        group_size = pow(2, i)\n",
    "        wbuffer = buffers.WriteBuffer(dir2, last_file, last_file + 2 * group_size, n, MAX_DICT_SIZE)\n",
    "        h1 = buffers.ReadBuffer(dir1, last_file, last_file + group_size, n)\n",
    "        last_file += group_size\n",
    "        h2 = buffers.ReadBuffer(dir1, last_file, last_file + group_size, n)\n",
    "        last_file += group_size\n",
    "\n",
    "        while not h1.over and not h2.over:\n",
    "            h = h1\n",
    "            if h2.word < h1.word or (h1.word == h2.word and h2.docID <= h1.docID):\n",
    "                h = h2\n",
    "            elem = h.get_next_pair()\n",
    "            if i == ceil(log2(n)) -1:\n",
    "                print(elem)\n",
    "            # wbuffer.insert(elem[0], elem[1])\n",
    "            if not wbuffer.insert(elem[0], elem[1]):\n",
    "                print(\"ERROR AT \" + str(i) + \", \" + str(j))\n",
    "\n",
    "        while not h1.over:\n",
    "            elem = h1.get_next_pair()\n",
    "            if i == ceil(log2(n)) -1:\n",
    "                print(elem)\n",
    "            # wbuffer.insert(elem[0], elem[1])\n",
    "            if not wbuffer.insert(elem[0], elem[1]):\n",
    "                print(\"ERROR AT \" + str(i) + \", \" + str(j))\n",
    "\n",
    "        while not h2.over:\n",
    "            elem = h2.get_next_pair()\n",
    "            if i == ceil(log2(n)) -1:\n",
    "                print(elem)\n",
    "            # wbuffer.insert(elem[0], elem[1])\n",
    "            if not wbuffer.insert(elem[0], elem[1]):\n",
    "                print(\"ERROR AT \" + str(i) + \", \" + str(j))\n",
    "\n",
    "        wbuffer.end_round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Final merge in 1 file\n",
    "# test:\n",
    "final_dict = {}\n",
    "dir = dir2\n",
    "\n",
    "for name in files_to_merge:\n",
    "    filename = dir + name.split(\"/\")[-1]\n",
    "    with open(filename, \"rb\") as f:\n",
    "        dict = pickle.load(f)\n",
    "    final_dict.update(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **tf-idf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tf-idf\n",
    "idfs = {} # muy grande, tal vez pasarlo a mem secundaria\n",
    "for key, postings_list in final_dict.items():\n",
    "    dfi = len(postings_list)\n",
    "    idfs[key] = log2(doc_count / dfi)\n",
    "    for i in range(len(postings_list)):\n",
    "        # print(postings_list[i])\n",
    "        docID, count = postings_list[i]\n",
    "        tf = log2(1 + count)\n",
    "        idf = idfs[key]\n",
    "        tfidf = tf * idf\n",
    "        postings_list[i] = (docID, tfidf)\n",
    "with open(final + index_filename, \"wb\") as f:\n",
    "    pickle.dump(final_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular las normas de cada documento\n",
    "norms = {doc : 0 for doc in files}\n",
    "for term, postings_list in final_dict.items():\n",
    "    for docid, tfidf in postings_list:\n",
    "        norms[docid] += (tfidf ** 2)\n",
    "for term in norms:\n",
    "    norms[term] = np.sqrt(norms[term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lyrics/dance_little_liar.txt', np.float64(0.3360896328029296)),\n",
       " ('lyrics/when_the_sun_goes_down.txt', np.float64(0.024473499789036634)),\n",
       " ('lyrics/4_out_of_5.txt', np.float64(0.0)),\n",
       " ('lyrics/505.txt', np.float64(0.0)),\n",
       " ('lyrics/arabella.txt', np.float64(0.0)),\n",
       " ('lyrics/batphone.txt', np.float64(0.0)),\n",
       " ('lyrics/cornerstone.txt', np.float64(0.0)),\n",
       " ('lyrics/no1_party_anthem.txt', np.float64(0.0)),\n",
       " ('lyrics/one_for_the_road.txt', np.float64(0.0))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(query_runner)\n",
    "qr = query_runner.QueryRunner(final + index_filename, files, idfs, norms, stoplist)\n",
    "query = \"when there's dirt beneath the dirt\"\n",
    "qr.make_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
